{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation of PDF analytics   \n",
    "\n",
    "Often I need to massively work with PDF documents and extract information from them for analysis. \n",
    "Some times it's export from some software, sometimes it's collection of documents which has been filled up over time.\n",
    "\n",
    "Often it can be hundreds or thousands of documents which needs to be:\n",
    "- separated by type of the document: \n",
    "    - Often when you working with some migration or process adjustment, you come up with task of \"cleanup documents\" in folder \n",
    "      or place where everyone was dumping files. Relying on name of the file or size of it or even count of pages is not good, \n",
    "      but often documents have header which can be extracted and based on that you can make assumption of type of the document.\n",
    "- create naming convention:\n",
    "    - in some cases you have hundreds of files like NDA.pdf, NDA1.pdf, NDA (1).pdf, NDA-1999.pdf, NDA-Google.pdf.\n",
    "      And you need to verify if it's correct file, and if for example it has a customer id number in it or date\n",
    "      extract this information and rename all files to NDA-1234586-20240807.pdf and if the file is not NDA - then \n",
    "      rename it to \"Unknown - %old name%.pdf\".\n",
    "\n",
    "[pymupdf](https://pypi.org/project/PyMuPDF/) has a really nice feature, which is not often described - convert PDF to JSON! \n",
    "After that you can work with file as a structural data and manipulate with everything from the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some PDF! In this case will generate PDF from URL using another great tool for converting HTML to PDF - [pdfkit](https://pypi.org/project/pdfkit/) \n",
    "and builtin function \".from_url\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pdfkit #apt-get install wkhtmltopdf\n",
    "pdfkit.from_url('https://en.wikipedia.org/wiki/Main_Page', 'out.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have PDF and some fields in it, which might be interested for us, such as:\n",
    "1. we want to make sure that it's actually Wiki home page, based on \"Welcome to Wikipedia\" text\n",
    "2. we want to know the date of this extraction (based on date mentioned on second page of the file)\n",
    "3. we want to know how many articles Wikipedia had on that day\n",
    "\n",
    "![image](Code_FAFCREvtn9.png)\n",
    "![image](Code_ukgsW7FH9s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we have a PDF, let's see how does it looks in JSON format.\n",
    "\n",
    "For that let's use pymupdf and [.get_text](https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_text) which has parameter \"json\"!\n",
    "\n",
    "In this example, I will choose only first page (doc[0]) and will print out formatted json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"width\": 595.0,\n",
      "    \"height\": 842.0,\n",
      "    \"blocks\": [\n",
      "        {\n",
      "            \"number\": 0,\n",
      "            \"type\": 1,\n",
      "            \"bbox\": [\n",
      "                20.69948959350586,\n",
      "                144.0253143310547,\n",
      "                90.43045043945312,\n",
      "                237.9604034423828\n",
      "            ],\n",
      "            \"width\": 121,\n",
      "            \"height\": 163,\n",
      "            \"ext\": \"jpeg\",\n",
      "            \"colorspace\": 3,\n",
      "            \"xres\": 96,\n",
      "            \"yres\": 96,\n",
      "            \"bpc\": 8,\n",
      "            \"transform\": [\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import pymupdf #pip install pymupdf\n",
    "import json\n",
    "\n",
    "doc = pymupdf.open(\"out.pdf\") \n",
    "jpage = json.loads(doc[0].get_text(\"json\"))\n",
    "\n",
    "print(json.dumps(jpage, indent=4)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find \"Welcome to Wikipedia\" in it and make sure that it's there:\n",
    "\n",
    "        $ ./parse_pdf.py | less -S\n",
    "        ...\n",
    "            {\n",
    "            \"number\": 8,\n",
    "            \"type\": 0,\n",
    "            \"bbox\": [\n",
    "                200.50161743164062,\n",
    "                52.73360824584961,\n",
    "                393.5578308105469,\n",
    "                70.3363037109375\n",
    "            ],\n",
    "            \"lines\": [\n",
    "                {\n",
    "                    \"spans\": [\n",
    "                        {\n",
    "                            \"size\": 14.983510971069336,\n",
    "                            \"flags\": 20,\n",
    "                            \"font\": \"DejaVuSerif-Bold\",\n",
    "                            \"color\": 0,\n",
    "                            \"ascender\": 0.93896484375,\n",
    "                            \"descender\": -0.23583984375,\n",
    "                            \"text\": \"Welcome to Wikipedia\",\n",
    "                            \"origin\": [\n",
    "                                200.50161743164062,\n",
    "                                66.80259704589844\n",
    "                            ],\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we found that text in block 8, in the first line and span. You can verify it by using jq or any other tools:\n",
    "\n",
    "        $ ./parse_pdf.py | jq '.blocks[8].lines[0].spans[0].text'\n",
    "        \"Welcome to Wikipedia\"\n",
    "\n",
    "Or you can do the same in python code directly with other elements we required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Wikipedia\n",
      "6,864,586 articles in English\n",
      "August 8\n"
     ]
    }
   ],
   "source": [
    "print(jpage[\"blocks\"][8][\"lines\"][0][\"spans\"][0][\"text\"])\n",
    "#Welcome to Wikipedia\n",
    "\n",
    "print(jpage[\"blocks\"][10][\"lines\"][0][\"spans\"][0][\"text\"])\n",
    "#6,864,026 articles in English\n",
    "\n",
    "print(jpage[\"blocks\"][17][\"lines\"][0][\"spans\"][0][\"text\"])\n",
    "#\"August 7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often PDF has same structure, like block->lines->spans->text and I have a small function to deal with it, \n",
    "plus provide some cleaning for outgoing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_by_key(json: str, block:int, line:int, span:int) -> str:\n",
    "    try:\n",
    "        return json[\"blocks\"][block][\"lines\"][line][\"spans\"][span][\"text\"].replace('\\xa0', ' ').strip()\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same code with it will look the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_wiki='Welcome to Wikipedia' count_of_articles='6,864,586 articles in English' date_of_extract='August 8'\n"
     ]
    }
   ],
   "source": [
    "is_wiki =           get_value_by_key(jpage,8,0,0) #Welcome to Wikipedia\n",
    "count_of_articles = get_value_by_key(jpage,10,0,0) #6,864,026 articles in English\n",
    "date_of_extract =   get_value_by_key(jpage,17,0,0) #\"August 7\"\n",
    "\n",
    "print(f\"{is_wiki=} {count_of_articles=} {date_of_extract=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some data clean up, logic and formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_wiki='yes' count_of_articles='6864586' date_of_extract=datetime.datetime(2024, 8, 8, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "is_wiki=\"yes\" if get_value_by_key(jpage,8,0,0) == \"Welcome to Wikipedia\" else \"no\"\n",
    "count_of_articles=get_value_by_key(jpage,10,0,0).split()[0].replace(',', '') #6,864,026 articles in English => 6864026\n",
    "date_of_extract=datetime.datetime.strptime(get_value_by_key(jpage,17,0,0), \"%B %d\").replace(year=2024)  #\"August 7\" => 2024-08-07\n",
    "\n",
    "print(f\"{is_wiki=} {count_of_articles=} {date_of_extract=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's apply it to all pdf files in the directory, plus let's collect all this data in dataframe so we can work with it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  is_wiki count_of_articles date_of_extract         file\n",
      "0     yes           6864586      2024-08-08  out-old.pdf\n",
      "1     yes           6864586      2024-08-08      out.pdf\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import pymupdf\n",
    "import json\n",
    "from sorcery import dict_of #pip install sorcery\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def get_value_by_key(json: str, block:int, line:int, span:int) -> str:\n",
    "    try:\n",
    "        return json[\"blocks\"][block][\"lines\"][line][\"spans\"][span][\"text\"].replace('\\xa0', ' ').strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extracts_info_from_pdf(file) -> dict:\n",
    "    out={}\n",
    "\n",
    "    doc = pymupdf.open(file) \n",
    "    jpage = json.loads(doc[0].get_text(\"json\"))\n",
    "\n",
    "    is_wiki=\"yes\" if get_value_by_key(jpage,8,0,0) == \"Welcome to Wikipedia\" else \"no\"\n",
    "    count_of_articles=get_value_by_key(jpage,10,0,0).split()[0].replace(',', '') #6,864,026 articles in English => 6846026\n",
    "    date_of_extract=datetime.datetime.strptime(get_value_by_key(jpage,17,0,0), \"%B %d\").replace(year=2024)  #\"August 7\" => 2024-08-07\n",
    "\n",
    "    out.update(dict_of(is_wiki, \n",
    "                       count_of_articles, \n",
    "                       date_of_extract,\n",
    "                       file))\n",
    "\n",
    "    return out    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"*.pdf\"\n",
    "    extracts=[] \n",
    "    for file in glob.glob(path, recursive=True):\n",
    "        extracts.append(extracts_info_from_pdf(file))\n",
    "\n",
    "    df_extracts = pd.DataFrame(extracts)\n",
    "    print(df_extracts)\n",
    "    #  is_wiki count_of_articles date_of_extract    file\n",
    "    #0     yes           6864586      2024-08-08 out.pdf\n",
    "    \n",
    "    #optionally to save into CSV: df_extracts.to_csv(\"extracts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
